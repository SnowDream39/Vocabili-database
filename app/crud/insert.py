
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, delete, and_, update, delete, insert, values, column, Integer, String
from sqlalchemy.dialects.postgresql import insert as insert
from sqlalchemy.exc import IntegrityError

from app.models import Song, Producer, Synthesizer, Vocalist, Uploader, Video, song_producer, song_synthesizer, song_vocalist, Snapshot, Ranking
from app.utils.misc import make_duration_int
from app.crud.update import update_video_streaks

from ..utils import validate_excel, read_excel, ensure_columns, normalize_nullable_int_columns, normalize_nullable_str_columns
from ..utils.filename import generate_board_file_path
from ..utils.cache import Cache

import pandas as pd
from datetime import datetime, timedelta, date
import math
from collections import namedtuple
import asyncio

BATCH_SIZE = 100

# =================  æ¯”è¾ƒå°çš„æ“ä½œï¼Œä¸å¯¹å¤–å…¬å¼€  ====================

async def resolve_changed_names(
    session: AsyncSession, 
    df: pd.DataFrame,
    cache: Cache | None = None
):
    if not cache:
        cache = Cache()
    await cache.ensure_loaded(session, ['song_map', 'video_map'])
    

    song_map = cache.song_map        # {name -> song_id}
    video_map = cache.video_map      # {bvid -> song_id}

    new_song_names = []             # éœ€è¦æ–°å»ºçš„Songåç§°
    video_updates = []              # (bvid, new_song_id)

    # === ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦åˆ›å»ºçš„æ–° name ===
    for _, row in df.iterrows():
        bvid = row["bvid"]
        new_name = row["name"]

        if bvid not in video_map:
            continue

        old_song_id = video_map[bvid]

        # å¦‚æœæ–°åå­—å·²ç»æœ‰å…³è”ï¼Œåˆ™çœ‹çœ‹æ˜¯å¦ç­‰äº old_song_id
        if new_name in song_map:
            if song_map[new_name] != old_song_id:
                # Video çš„ song_id è¦æ”¹åˆ°æ–°çš„ song_id
                video_updates.append((bvid, song_map[new_name]))
            continue

        # æ–°åå­—ä¸å­˜åœ¨ â†’ åç»­è¦åˆ›å»ºä¸€ä¸ªæ–°çš„ Song
        new_song_names.append(new_name)

    # å»é‡
    new_song_names = list(set(new_song_names))

    try:
        # === ç¬¬äºŒæ­¥ï¼šæ‰¹é‡æ’å…¥æ–°çš„ Song ===
        created_name_to_id = {}
        if new_song_names:
            insert_data = [{"name": name} for name in new_song_names]
            stmt = insert(Song).returning(Song.id, Song.name)
            rows = (await session.execute(stmt, insert_data)).fetchall()

            for sid, name in rows:
                created_name_to_id[name] = sid
                cache.song_map[name] = sid  # æ›´æ–°ç¼“å­˜

        # === ç¬¬ä¸‰æ­¥ï¼šå¯¹æ‰€æœ‰ Video æ›´æ–° song_id ===
        # å†æ‰§è¡Œ df éå†ä¸€æ¬¡ï¼Œæ‰¾å‡ºå› æ–° Song åˆ›å»ºè€Œéœ€è¦æ›´æ–°çš„é¡¹
        for _, row in df.iterrows():
            bvid = row["bvid"]
            new_name = row["name"]

            if bvid not in video_map:
                continue

            # å¦‚æœ name å±äºæ–°åˆ›å»ºçš„ Songï¼Œåˆ™æ›´æ–°åˆ°æ–° id
            if new_name in created_name_to_id:
                new_id = created_name_to_id[new_name]
                video_updates.append((bvid, new_id))
                continue

        # æ‰§è¡Œ update
        for bvid, new_song_id in video_updates:
            stmt = (
                update(Video)
                .where(Video.bvid == bvid)
                .values(song_id=new_song_id)
            )
            await session.execute(stmt)

            # ç¼“å­˜åŒæ­¥æ›´æ–°
            video_map[bvid] = new_song_id

        await session.commit()

        return {
            "created_songs": len(new_song_names),
            "updated_videos": len(video_updates),
        }
    except IntegrityError as e:
        await session.rollback()
        print("æ’å…¥æ•°æ®å‡ºé”™:", e)
        raise e


async def insert_artists(
    session: AsyncSession, 
    df,
    cache: Cache | None = None
    ):
    if not cache:
        cache = Cache()
    await cache.ensure_loaded(session, ['artist_maps'])

    artist_map = {
        Producer: set(),
        Synthesizer: set(),
        Vocalist: set(),
        Uploader: set(),
    }

    for _, row in df.iterrows():
        for table, col in (
            (Producer, 'author'),
            (Synthesizer, 'synthesizer'),
            (Vocalist, 'vocal'),
        ):
            if not pd.isna(row[col]):
                for name in row[col].split('ã€'):
                    artist_map[table].add(name)
                    
        if not pd.isna(row['uploader']):  # ä¸Šä¼ è€…
            artist_map[Uploader].add(row['uploader'])

    for table, names in artist_map.items():
        if not names:
            continue
        new_names = names - cache.artist_maps[table].keys()
        if (len(new_names) > 0):
            print(f"{table.__tablename__} åˆ›å»ºartistï¼š{new_names}")

        if new_names:
            # æ„é€ è¦æ’å…¥çš„æ•°æ®
            values = [{"name": name} for name in new_names]
            stmt = insert(table).values(values).on_conflict_do_nothing().returning(table.name, table.id)
            result = await session.execute(stmt)
            records = result.all()
            cache.artist_maps[table].update({r[0]: r[1] for r in records})


    await session.flush()

async def insert_songs(session: AsyncSession, df, cache: Cache | None = None):
    if not cache:
        cache = Cache()

    ensure_columns(df, ['image_url'])
    await cache.ensure_loaded(session, ['song_map'])

    SongRecord = namedtuple('SongRecord', ['name', 'type'])
    UpdateSongRecord = namedtuple('UpdateSongRecord', ['id', 'type'])

    song_records: list[SongRecord] = []
    for row in df.to_dict(orient='records'):
        name = row['name']
        song_type = row['type'] if not pd.isna(row['type']) else None
        if not pd.isna(name):
            song_records.append(SongRecord(name, song_type))

    # ğŸ”§ æ–°å¢ï¼šname -> type æ˜ å°„ï¼ˆåé¢çš„å”¯ä¸€æ•°æ®æ¥æºï¼‰
    name_type_map = {
        r.name: r.type
        for r in song_records
    }

    song_names = set(name_type_map.keys())
    existing_song_names = set(cache.song_map.keys())

    new_song_names = song_names - existing_song_names
    update_song_names = song_names & existing_song_names

    # âœ… ä¿®å¤ 1ï¼šæ­£ç¡®æ„é€  new_song_records
    new_song_records = [
        SongRecord(name, name_type_map[name])
        for name in new_song_names
    ]

    if new_song_records:
        stmt = (
            insert(Song)
            .values(
                [
                    {
                        "name": s.name,
                        "type": s.type
                    }
                    for s in new_song_records
                ]
            )
            .returning(Song.id, Song.name)
        )

        result = await session.execute(stmt)
        rows = result.all()
        cache.song_map.update({name: id for id, name in rows})

    # âœ… ä¿®å¤ 2ï¼šæ­£ç¡®æ„é€  update_song_records
    update_song_records = [
        UpdateSongRecord(cache.song_map[name], name_type_map[name])
        for name in update_song_names
    ]

    if update_song_records:
        v = (
            values(
                column("id", Integer),
                column("type", String)
            )
            .data(
                [(s.id, s.type) for s in update_song_records]
            )
            .alias("v")
        )

        await session.execute(
            update(Song)
            .where(Song.id == v.c.id)
            .values(type=v.c.type)
        )




async def insert_relations(
    session: AsyncSession,
    df: pd.DataFrame,
    strict: bool,
    new_songs: list,
    cache: Cache | None = None
    ):
    if not cache:
        cache = Cache()
    await cache.ensure_loaded(session, ['song_map', 'artist_maps', 'song_artist_maps'])
       
    new_song_names = list(map(lambda x: x[0], new_songs))
    new_song_df = df.loc[df['name'].isin(new_song_names)][['name', 'synthesizer', 'author', 'vocal']].copy()
    
    # éä¸¥æ ¼æ¨¡å¼ä¸‹å†æ£€æŸ¥ä¸€éä¸éœ€è¦æ’å…¥çš„å†…å®¹
    if not strict:
        new_song_df = new_song_df.loc[new_song_df['synthesizer'].notna() & new_song_df['author'].notna() & new_song_df['vocal'].notna()].copy()
    
    for cls, table, field in (
        (Producer, song_producer, 'author'),
        (Synthesizer, song_synthesizer, 'synthesizer'),
        (Vocalist, song_vocalist, 'vocal')
    ):
        rel_df = (
            new_song_df[['name', field]]
            .assign( song_id=lambda df: df['name'].map(cache.song_map))
            .copy()
        )
        
        rel_df[field] = rel_df[field].astype(str).str.split('ã€')
        rel_df = rel_df.explode(field)
        rel_df = rel_df[rel_df[field].notna()]
        
        rel_df['artist_id'] = rel_df[field].map(cache.artist_maps[cls])
        rel_df = rel_df[rel_df['artist_id'].notna()]
        

        rel_records = set(rel_df[['song_id', 'artist_id']].itertuples(index=False, name=None))

        existing_rel_set = set(tuple(x) for x in cache.song_artist_maps[cls])
        new_rel_records = list(rel_records - existing_rel_set)

        if new_rel_records:
            new_rel_dicts = [{'song_id': t[0], 'artist_id': t[1]} for t in new_rel_records]
            stmt = insert(table).values(new_rel_dicts).on_conflict_do_nothing()
            await session.execute(stmt)
            cache.song_artist_maps[cls].update(new_rel_records)
            
            
async def update_relations(
    session: AsyncSession,
    df: pd.DataFrame,
    cache: Cache | None = None
    ):
    """
    æ›´æ–°å…¨éƒ¨artistå…³ç³»
    """
    if not cache:
        cache = Cache()
    await cache.ensure_loaded(session, ['song_map', 'artist_maps'])
       
    song_df = df[['name', 'synthesizer', 'author', 'vocal']].copy()
    
    for cls, table, field in (
        (Producer, song_producer, 'author'),
        (Synthesizer, song_synthesizer, 'synthesizer'),
        (Vocalist, song_vocalist, 'vocal')
    ):
        rel_df = (
            song_df[['name', field]]
            .assign( song_id=lambda df: df['name'].map(cache.song_map))
            .copy()
        )
        
        rel_df = rel_df[rel_df['song_id'].notna()]
        rel_df = rel_df[rel_df[field].notna()].copy()
        rel_df[field] = rel_df[field].astype(str).str.split('ã€')
        rel_df = rel_df.explode(field)
        rel_df['artist_id'] = rel_df[field].map(cache.artist_maps[cls])
        rel_df = rel_df[rel_df['artist_id'].notna()].copy()

        song_ids = rel_df['song_id'].unique().tolist()
        if song_ids:
            stmt = delete(table).where(table.c.song_id.in_(song_ids))
            await session.execute(stmt)
            

        rel_records = set(map(tuple, rel_df[['song_id', 'artist_id']].to_numpy()))


        if rel_records:
            new_rel_dicts = [{'song_id': t[0], 'artist_id': t[1]} for t in rel_records]
            stmt = insert(table).values(new_rel_dicts).on_conflict_do_nothing()
            await session.execute(stmt)


async def insert_videos(
    session: AsyncSession, 
    df: pd.DataFrame, 
    update: bool = False,
    cache: Cache | None = None
    ):

    """
    æ’å…¥è§†é¢‘ã€‚å†²çªæ›´æ–°ã€‚
    
    å¦‚æœæ­Œæ›²ä¸å­˜åœ¨å°±ä¸æ’å…¥ã€‚
    
    å¦‚æœæ•°æ®é‡Œé¢æ²¡æœ‰image_urlï¼Œä¸ä¼šæ›´æ–°ã€‚
    """
    
    if not cache:
        cache = Cache()
    
    has_thumbnail = 'image_url' in df.columns
    use_cols = ['bvid', 'title', 'pubdate', 'duration', 'page', 'song_id', 'uploader_id', 'copyright']
    update_cols = ['title', 'pubdate', 'uploader_id', 'duration', 'page', 'copyright', 'thumbnail']
    
    normalize_nullable_int_columns(df, ['page', 'copyright'])
    normalize_nullable_str_columns(df, ['duration', 'title'])
    await cache.ensure_loaded(session, ['video_map', 'song_map', 'artist_maps'])
    df = df.assign(
        song_id = lambda d: d['name'].map(cache.song_map),
        uploader_id = lambda d: d['uploader'].map(cache.artist_maps[Uploader]),
        duration = lambda d: d['duration'].map(make_duration_int)
    )
    
    if has_thumbnail:  
        df = df.rename(columns={'image_url': 'thumbnail'})
        use_cols.append('thumbnail')
        normalize_nullable_str_columns(df, ['thumbnail'])
    
    df = df.loc[df['song_id'].notna()][use_cols].copy()
    
    normalize_nullable_int_columns(df, ['uploader_id'])
    
    # è½¬æ¢æˆ record dict
    records = df.to_dict(orient="records")


    # ----------- UPSERTï¼ˆæ ¸å¿ƒï¼‰-----------
    if update:
        excluded = insert(Video).excluded
        stmt = (
            insert(Video)
            .values(records)
            .on_conflict_do_update(
                index_elements=["bvid"],
                set_={
                    field: excluded[field] for field in update_cols
                }
            )
        )
    else:
        stmt = insert(Video).values(records).on_conflict_do_nothing(
            index_elements=["bvid"]
        )
        
        
    await session.execute(stmt)

    # ----------- æ›´æ–° cache -----------
    for row in records:
        cache.video_map[row["bvid"]] = row["song_id"]

        
# =============   ç›´æ¥è¢«è°ƒç”¨çš„æ“ä½œ  =========
        

    
async def execute_import_snapshots(
    session: AsyncSession,
    date: str,
    strict: bool,
    cache: Cache | None = None
    ):
    if not cache:
        cache = Cache()
    date_ = datetime.strptime(date, "%Y-%m-%d")
    df = read_excel(f'./data/æ•°æ®/{date_.strftime("%Y%m%d")}.xlsx').assign(date=date_)
    
    if strict:
        validate_excel(df)
        
    # ---------- åŸæœ‰è®°å½•æ¸…ç©º -------------
    delete_stmt = delete(Snapshot).where(
        Snapshot.date == date_
    )    
    await session.execute(delete_stmt)

    try:
        total = len(df)
        for start in range(0, total, BATCH_SIZE):
            end = start + BATCH_SIZE
            batched_df = df.iloc[start:end].copy()
            await insert_videos(session, batched_df, False, cache)

            batched_df = batched_df[batched_df['bvid'].isin(cache.video_map.keys())]
            if not batched_df.empty:
            # -------- æ’å…¥æ•°æ®è®°å½• ---------
                snapshots = batched_df[["bvid", "date", "view", "favorite", "coin", "like"]].to_dict(orient="records")
                stmt = insert(Snapshot).values(snapshots).on_conflict_do_update(
                    index_elements=['bvid', 'date'],
                    set_={field: insert(Snapshot).excluded[field] for field in ['view', 'favorite', 'coin', 'like']}
                )
                await session.execute(stmt)
                await session.flush()
                await session.commit()
        
        # ------------ æ›´æ–° streak ------------
        await update_video_streaks(session, date_)
    except IntegrityError as e:
        await session.rollback()
        print("æ’å…¥æ•°æ®å‡ºé”™:", e)
        raise e
    
    
    
async def execute_import_rankings(
    session: AsyncSession, 
    board: str, 
    part: str, 
    issue: int,  
    strict: bool,
    cache: Cache | None = None
    ):
    if not cache:
        cache = Cache()

    delete_stmt = delete(Ranking).where(
        (Ranking.board == board) &
        (Ranking.part == part) &
        (Ranking.issue == issue)
    )
    
    await session.execute(delete_stmt)

    
    df: pd.DataFrame = read_excel(
        generate_board_file_path(board, part, issue),
    ).assign( board = board, part = part, issue = issue)
    
    if strict:
        # ä¸¥æ ¼æ¨¡å¼çš„æ„ä¹‰å°±åœ¨äºè¿™é‡Œæœ‰éªŒè¯
        # éªŒè¯è¿‡åï¼Œè¿˜æ˜¯æŒ‰ç…§ä¸€èˆ¬é‚£æ ·ï¼Œå¾ˆå¤šå­—æ®µå…è®¸null
        errors = validate_excel(df)
        if len(errors) >= 1:
            raise Exception("\n".join(errors))
        yield "event: progress\ndata: æ•°æ®éªŒè¯é€šè¿‡\n\n"

    try:
        
        total = len(df)
        total_batches = math.ceil(total / BATCH_SIZE)
        for i in range(total_batches):
            yield f"event: progress\ndata: æ­£åœ¨æ‰§è¡Œç¬¬ {i+1}/{total_batches} æ‰¹æ¬¡...\n\n"
            start = i * BATCH_SIZE
            end = start + BATCH_SIZE
            batch_df = df.iloc[start: end].copy()
            print(f"{start} ~ {end}")
            if (part != 'new' and board in ['vocaloid-daily', 'vocaloid-weekly']):
                await resolve_changed_names(session, batch_df, cache)
                await insert_artists(session, batch_df, cache)
                await insert_songs(session, batch_df, cache)
                await update_relations(session, batch_df, cache)
                await insert_videos(session, batch_df, True, cache)

                insert_df = batch_df.assign(
                    board=board,
                    issue=issue,
                    part=part
                )[['board', 'part', 'issue', 'rank','bvid','count','point','view','favorite','coin','like','view_rank','favorite_rank','coin_rank','like_rank']]
                insert_df['count'] = insert_df['count'].astype("Int64")
                insert_df['song_id'] = insert_df['bvid'].map(cache.video_map)
            
            else: 
                await insert_videos(session, batch_df, False, cache)
                insert_df = batch_df.assign(
                    board=board,
                    issue=issue,
                    part=part
                )[['board', 'part', 'issue', 'rank','bvid','point','view','favorite','coin','like','view_rank','favorite_rank','coin_rank','like_rank']]
                insert_df['song_id'] = insert_df['bvid'].map(cache.video_map)

            insert_df = insert_df.dropna(subset=['song_id'])
            insert_df = insert_df.replace({pd.NA: None})
            records = insert_df.to_dict(orient='records')
            
            insert_stmt = insert(Ranking).values(records).on_conflict_do_nothing()
            await session.execute(insert_stmt)
            await session.commit()
            
        yield "event: complete\ndata: å®Œæˆ\n\n"
    
    except IntegrityError as e:
        await session.rollback()
        print("æ’å…¥æ•°æ®å‡ºé”™:", e)
    

